# Implementing a Diffusion Language Model (LLaDA) – A Practical Guide

## Overview of Diffusion Language Models vs Autoregressive Models
Diffusion-based language models generate text by *iteratively refining a noised sequence*, rather than predicting one token at a time left-to-right as in autoregressive models (ARMs). In LLaDA (Large Language Diffusion with mAsking) ([Large Language Diffusion Models](https://ml-gsai.github.io/LLaDA-demo/#:~:text=We%20introduce%20LLaDA%20,demonstrates%20the%20aforementioned%20remarkable%20capabilities)) ([Diffusion Over Autoregression. Explore the paper that highlights the… | by Anay Dongre | Mar, 2025 | Towards AI](https://pub.towardsai.net/diffusion-over-autoregression-5e3e4e160470?source=rss----98111c9905da---4#:~:text=LLaDA%20replaces%20the%20autoregressive%20paradigm,breaking%20free%20from%20autoregression%E2%80%99s%20constraints)), noise is introduced by masking tokens in a sequence, and the model learns to denoise (reconstruct) the original text. This paradigm allows the model to utilize *bidirectional context* (like BERT) during generation and potentially generate multiple tokens in parallel, addressing some AR limitations in speed and coherence ([Diffusion Over Autoregression. Explore the paper that highlights the… | by Anay Dongre | Mar, 2025 | Towards AI](https://pub.towardsai.net/diffusion-over-autoregression-5e3e4e160470?source=rss----98111c9905da---4#:~:text=Autoregressive%20models%20like%20GPT,This%20setup%20has%20limitations)) ([Diffusion Over Autoregression. Explore the paper that highlights the… | by Anay Dongre | Mar, 2025 | Towards AI](https://pub.towardsai.net/diffusion-over-autoregression-5e3e4e160470?source=rss----98111c9905da---4#:~:text=LLaDA%20replaces%20the%20autoregressive%20paradigm,breaking%20free%20from%20autoregression%E2%80%99s%20constraints)).

Key differences in diffusion LMs:
- **No Left-to-Right Constraint:** The model can look at the entire sequence (except masked parts) at once, enabling coherence by utilizing future context during generation ([Diffusion Over Autoregression. Explore the paper that highlights the… | by Anay Dongre | Mar, 2025 | Towards AI](https://pub.towardsai.net/diffusion-over-autoregression-5e3e4e160470?source=rss----98111c9905da---4#:~:text=LLaDA%20replaces%20the%20autoregressive%20paradigm,breaking%20free%20from%20autoregression%E2%80%99s%20constraints)).
- **Iterative Generation:** Text is produced through multiple *denoising steps* instead of a single pass. The model starts from a fully masked sequence and gradually fills in tokens over a number of steps ([[2502.09992] Large Language Diffusion Models](https://ar5iv.org/html/2502.09992v2#:~:text=Image%3A%20Refer%20to%20caption%20Figure,step%20with%20flexible%20remask%20strategies)).
- **Trade-off in Speed:** By filling in many tokens per step, diffusion LMs can generate text in fewer iterations than ARMs (which always need as many steps as tokens). In principle, a diffusion LM can approach or even exceed the speed of AR decoding by increasing parallelism (fewer steps) ([@Kseniase on Hugging Face: "5 New implementations of Diffusion Models

Diffusion models are widely used…"](https://huggingface.co/posts/Kseniase/475779328543857#:~:text=1,1%2C000%20tokens%2Fsec%20on%20NVIDIA%20H100s)) ([@Kseniase on Hugging Face: "5 New implementations of Diffusion Models

Diffusion models are widely used…"](https://huggingface.co/posts/Kseniase/475779328543857#:~:text=3.%20LLaDA%20,4o%20in%20reversal%20poetry)). However, each diffusion step processes the whole sequence, so practical speed-ups require careful tuning or efficient parallel computation.

In summary, diffusion LMs like LLaDA replace the one-token-at-a-time paradigm with a mask-and-refine process, offering a *principled generative modeling approach* (maximizing data likelihood via an ELBO) that has been shown to match key capabilities of AR LLMs ([@Kseniase on Hugging Face: "5 New implementations of Diffusion Models

Diffusion models are widely used…"](https://huggingface.co/posts/Kseniase/475779328543857#:~:text=3.%20LLaDA%20,4o%20in%20reversal%20poetry)).

## Core Ideas Behind the LLaDA Model

### Model Architecture 
LLaDA uses a **Transformer-based mask predictor** that is architecturally similar to a standard Transformer language model, with one crucial change: **no causal masking** ([[2502.09992] Large Language Diffusion Models](https://ar5iv.org/html/2502.09992v2#:~:text=LLaDA%20employs%20a%20Transformer%C2%A0,the%20entire%20input%20for%20predictions)). In practice, this means LLaDA’s network looks like a Transformer **encoder** (like BERT) rather than a decoder – it can attend bidirectionally over the sequence since it doesn’t need to prevent “seeing the future” ([LLaDA/GUIDELINES.md at main · ML-GSAI/LLaDA · GitHub](https://github.com/ML-GSAI/LLaDA/blob/main/GUIDELINES.md#:~:text=LLaDA%20employs%20a%20Transformer%20Encoder,causal%20mask%20from%20the%20self)). 

- The model operates over token sequences of a chosen vocabulary (in LLaDA’s case, they used a SentencePiece vocabulary and designated a special token ID as `[MASK]`) ([LLaDA/GUIDELINES.md at main · ML-GSAI/LLaDA · GitHub](https://github.com/ML-GSAI/LLaDA/blob/main/GUIDELINES.md#:~:text=LLaDA%20employs%20a%20Transformer%20Encoder,causal%20mask%20from%20the%20self)). This `[MASK]` token represents corrupted/noisy positions.
- Apart from the absence of a causal mask, the network architecture (self-attention layers, feed-forward networks, etc.) is a vanilla Transformer of the kind used in many LMs ([LLaDA/GUIDELINES.md at main · ML-GSAI/LLaDA · GitHub](https://github.com/ML-GSAI/LLaDA/blob/main/GUIDELINES.md#:~:text=LLaDA%20employs%20a%20Transformer%20Encoder,causal%20mask%20from%20the%20self)). If starting from an existing AR Transformer implementation, one can **remove the causal masking** in self-attention to convert it into LLaDA’s bidirectional mask predictor ([LLaDA/GUIDELINES.md at main · ML-GSAI/LLaDA · GitHub](https://github.com/ML-GSAI/LLaDA/blob/main/GUIDELINES.md#:~:text=LLaDA%20employs%20a%20Transformer%20Encoder,causal%20mask%20from%20the%20self)).
- *Time conditioning:* Notably, diffusion models often feed a time-step or noise-level embedding into the network. LLaDA’s design simplified this – the authors found that masked diffusion can work **without an explicit time embedding** (the model implicitly infers the “noise level” from how many tokens are masked) ([Large Language Diffusion Models - GitHub](https://github.com/ML-GSAI/LLaDA#:~:text=Large%20Language%20Diffusion%20Models%20,This%20insight%20provides%20the)). For a simple POC, you can initially **omit time-step embeddings**. The model will just take the masked sequence as input. (Optionally, one could concatenate a learned time embedding to token embeddings if experimenting with conditioning on diffusion step, but this isn’t strictly required ([Large Language Diffusion Models - GitHub](https://github.com/ML-GSAI/LLaDA#:~:text=Large%20Language%20Diffusion%20Models%20,This%20insight%20provides%20the)).)

**Why this architecture?** By using full sequence attention, the model can leverage all available context at each denoising step. This is akin to how BERT fills in blanks using both left and right context, but here it’s done repeatedly to gradually generate a whole sequence.

### Training Strategy (Forward Process and Objective)
Training LLaDA involves simulating the *noising (masking) process* on real text and teaching the model to recover the original tokens. This is very similar to a masked language modeling objective, with the twist that the *masking ratio varies from 0% to 100%* and is treated as a continuous diffusion time. The training setup is as follows:

- **Forward (Diffusion) Process:** Take a sequence of tokens from the training data and randomly choose a mask fraction `t` uniformly in [0,1]. Then **mask each token with probability `t`** (independently) to produce a corrupted sequence `x_t` ([[2502.09992] Large Language Diffusion Models](https://ar5iv.org/html/2502.09992v2#:~:text=Unlike%20ARMs%20in%20Eq,as%20moves%20from%20to)) ([LLaDA/GUIDELINES.md at main · ML-GSAI/LLaDA · GitHub](https://github.com/ML-GSAI/LLaDA/blob/main/GUIDELINES.md#:~:text=p_mask%20%3D%20%281%20,None%5D.repeat%281%2C%20l)). When `t=0`, no tokens are masked (no noise); when `t=1`, all tokens are masked (full noise). Most of the time, `x_t` will have some partial masking. In code, this can be done by sampling a matrix of Bernoulli mask decisions with probability `p_mask = t` for each token ([LLaDA/GUIDELINES.md at main · ML-GSAI/LLaDA · GitHub](https://github.com/ML-GSAI/LLaDA/blob/main/GUIDELINES.md#:~:text=p_mask%20%3D%20%281%20,None%5D.repeat%281%2C%20l)). For example, if `t=0.5`, about 50% of the tokens in the sequence are replaced with `[MASK]` symbols. This process is analogous to adding Gaussian noise in continuous diffusion – here we *add discrete noise by masking tokens* ([LLaDA - Large Language Diffusion Model](https://llada.pro/#:~:text=LLaDA%20works%20based%20on%20a,the%20original%20data%20from)).

- **Reverse (Denoising) Model:** The Transformer mask-predictor is trained to take the noised sequence `x_t` and predict the *original* tokens for all masked positions in one shot ([[2502.09992] Large Language Diffusion Models](https://ar5iv.org/html/2502.09992v2#:~:text=The%20core%20of%20LLaDA%20is,only%20on%20the%20masked%20tokens)). In other words, the model outputs a probability distribution over the vocabulary for each position, but we **only compute loss on the masked positions** (since unmasked tokens are already correct and just serve as context) ([[2502.09992] Large Language Diffusion Models](https://ar5iv.org/html/2502.09992v2#:~:text=The%20core%20of%20LLaDA%20is,only%20on%20the%20masked%20tokens)). The loss used is the standard cross-entropy for those masked tokens.

- **Loss Function as Likelihood Bound:** Importantly, the training objective is formulated as an ELBO maximizing the likelihood of data. In practice it reduces to a weighted masked token prediction loss. LLaDA’s loss for a token is weighted by the inverse of the masking probability at that token ([LLaDA/GUIDELINES.md at main · ML-GSAI/LLaDA · GitHub](https://github.com/ML-GSAI/LLaDA/blob/main/GUIDELINES.md#:~:text=noisy_batch%2C%20masked_indices%2C%20p_mask%20%3D%20forward_process,logits)). Intuitively, this means if a token was masked with a low probability (e.g. in a low-noise scenario), its prediction error is up-weighted, whereas errors in very high-noise scenarios are down-weighted. This weighting ensures each training example contributes fairly across different noise levels. The authors note that this objective is a **provable upper-bound on negative log-likelihood** of the true data distribution ([[2502.09992] Large Language Diffusion Models](https://ar5iv.org/html/2502.09992v2#:~:text=Once%20trained%2C%20we%20can%20simulate,2024)), making the approach *principled and theoretically sound* (not just a heuristic like standard BERT masking). In summary, the model is learning to “denoise” text optimally at any mask level.

- **Comparison to BERT:** A traditional BERT MLM uses a fixed mask ratio (like 15%) and does not aim to model a full generative process. LLaDA instead samples *all mask ratios 0–1 uniformly* ([[2502.09992] Large Language Diffusion Models](https://ar5iv.org/html/2502.09992v2#:~:text=Notably%2C%20LLaDA%20employs%20a%20masking,with%20large%20data%20and%20models)), including extreme cases like fully masked sequences, which teaches the model to generate text from nothing. This is crucial – it means after training, the model knows how to go from 100% masks to a coherent sequence (something BERT was never trained to do). Essentially, LLaDA’s training can be seen as covering a continuum of corruption levels, whereas BERT covered only mild corruption. That difference allows LLaDA to function as a generative LM, not just a fill-in-the-blank tool ([[2502.09992] Large Language Diffusion Models](https://ar5iv.org/html/2502.09992v2#:~:text=Notably%2C%20LLaDA%20employs%20a%20masking,with%20large%20data%20and%20models)).

**Training procedure:** In practice, implementing training is straightforward:
1. Sample a batch of text sequences (e.g. from Wikipedia or another corpus).
2. For each sequence, sample a uniform random `t`. Compute `p_mask = t*(1-ε) + ε` (the authors add a small ε~0 to avoid zero-masking) ([LLaDA/GUIDELINES.md at main · ML-GSAI/LLaDA · GitHub](https://github.com/ML-GSAI/LLaDA/blob/main/GUIDELINES.md#:~:text=p_mask%20%3D%20%281%20,None%5D.repeat%281%2C%20l)).
3. Mask tokens with probability `p_mask` to create the input batch `x_t` (replace masked positions with `[MASK]` token) ([LLaDA/GUIDELINES.md at main · ML-GSAI/LLaDA · GitHub](https://github.com/ML-GSAI/LLaDA/blob/main/GUIDELINES.md#:~:text=p_mask%20%3D%20%281%20,None%5D.repeat%281%2C%20l)).
4. Feed `x_t` into the Transformer. Get output logits for each token position.
5. Compute cross-entropy loss between the logits and the true tokens, *only at masked positions*. Furthermore, divide each token’s loss by `p_mask` (the mask probability for that position) ([LLaDA/GUIDELINES.md at main · ML-GSAI/LLaDA · GitHub](https://github.com/ML-GSAI/LLaDA/blob/main/GUIDELINES.md#:~:text=noisy_batch%2C%20masked_indices%2C%20p_mask%20%3D%20forward_process,logits)). Average these losses over the batch.
6. Backpropagate and update model parameters.

This process teaches the model to handle all levels of noise. Pseudocode for one training step, based on LLaDA’s open guidelines, looks like:

```python
# Given: input_ids (batch of tokenized sequences)
t = torch.rand((batch_size,), 0, 1)            # sample diffusion time for each sequence
p_mask = t.unsqueeze(1).expand(-1, seq_length) # mask prob for each token in sequence
mask_rand = torch.rand_like(input_ids_float)   # random mask selector
mask_positions = mask_rand < p_mask            # boolean mask of which tokens to mask
mask_token_id = special_token_id_for_MASK
noisy_input = input_ids.masked_fill(mask_positions, mask_token_id)  # apply masks

logits = model(noisy_input)  # forward pass (Transformer output logits for vocab at each pos)
# Compute loss only on masked positions:
target_tokens = input_ids[mask_positions]
pred_logits = logits[mask_positions]  # model predictions at masked positions
token_loss = cross_entropy(pred_logits, target_tokens, reduction='none')
# Apply weight 1/p_mask to each position’s loss:
token_loss = token_loss / p_mask[mask_positions]
loss = token_loss.mean()
```

This is essentially what the LLaDA code does during pre-training ([LLaDA/GUIDELINES.md at main · ML-GSAI/LLaDA · GitHub](https://github.com/ML-GSAI/LLaDA/blob/main/GUIDELINES.md#:~:text=noisy_batch%2C%20masked_indices%2C%20p_mask%20%3D%20forward_process,logits)). Even with this simple scheme, you are optimizing a valid likelihood objective for a diffusion LM ([[2502.09992] Large Language Diffusion Models](https://ar5iv.org/html/2502.09992v2#:~:text=Once%20trained%2C%20we%20can%20simulate,2024)).

### Decoding Approach (Iterative Sampling)
After training, text generation is done by **simulating the reverse diffusion process**: start from a fully masked sequence and iteratively replace masks with predicted tokens until you reach an all-token sequence. The general decoding algorithm is:

1. **Initialization:** Decide on the *length* of the output you want to generate (or a max length). If you have a **prompt or context**, place those prompt tokens at the start of the sequence and mask only the positions that need to be generated (e.g. the “response” part) ([[2502.09992] Large Language Diffusion Models](https://ar5iv.org/html/2502.09992v2#:~:text=Image%3A%20Refer%20to%20caption%20Figure,step%20with%20flexible%20remask%20strategies)). If doing unconditional generation, you can start with an empty prompt or a special beginning token and mask all other positions. For example, to generate 50 tokens of text with no prompt, you would start with 50 `[MASK]` tokens. (In practice, you might include an end-of-sequence token `<EOS>` in vocab to signal when text ends.)
2. **Iterative Refinement:** The model will perform a fixed number of *sampling steps* (let’s call this `T`). Each step consists of:
   - **Prediction:** Feed the current sequence (with some tokens filled in and others masked) into the Transformer. It will output probabilities for all positions that are still masked.
   - **Token sampling/filling:** For each masked position, choose a token from the predicted distribution. In a basic proof-of-concept, you could take the argmax (most likely token) for determinism, or sample randomly according to the probabilities for more diversity. This produces candidate tokens for all masked slots.
   - **Remasking (except final step):** Now decide which positions will remain masked for the *next* iteration. LLaDA allows *flexible remasking* strategies ([[2502.09992] Large Language Diffusion Models](https://ar5iv.org/html/2502.09992v2#:~:text=Image%3A%20Refer%20to%20caption%20Figure,step%20with%20flexible%20remask%20strategies)) ([LLaDA/GUIDELINES.md at main · ML-GSAI/LLaDA · GitHub](https://github.com/ML-GSAI/LLaDA/blob/main/GUIDELINES.md#:~:text=in%20our%20paper%2C%20the%20mask,confidence%20remasking.%20Notably%2C%20both)):
     - **Random Remasking:** Randomly keep a certain proportion of the tokens masked. For example, you might unmask (reveal) 50% of the masked tokens and re-mask the other 50% again. This means those re-masked positions will be predicted again in subsequent steps, allowing the model to revise its earlier guesses. Random selection is simple and was found to work robustly ([LLaDA/GUIDELINES.md at main · ML-GSAI/LLaDA · GitHub](https://github.com/ML-GSAI/LLaDA/blob/main/GUIDELINES.md#:~:text=in%20our%20paper%2C%20the%20mask,confidence%20remasking.%20Notably%2C%20both)) ([LLaDA/GUIDELINES.md at main · ML-GSAI/LLaDA · GitHub](https://github.com/ML-GSAI/LLaDA/blob/main/GUIDELINES.md#:~:text=than%20512,when%20randomly%20remasking%20is%20used)).
     - **Confidence-based Remasking:** Alternatively, use the model’s confidence: e.g. unmask tokens with high confidence (low entropy or high probability on one token) and re-mask tokens it was unsure about ([LLaDA/GUIDELINES.md at main · ML-GSAI/LLaDA · GitHub](https://github.com/ML-GSAI/LLaDA/blob/main/GUIDELINES.md#:~:text=in%20our%20paper%2C%20the%20mask,confidence%20remasking.%20Notably%2C%20both)). This focuses the model’s later refinement on the problematic spots. (The LLaDA authors tried this “low-confidence remasking” and found it can work well, though in some cases it led to issues like too eagerly fixing an `<EOS>` token in place ([LLaDA/GUIDELINES.md at main · ML-GSAI/LLaDA · GitHub](https://github.com/ML-GSAI/LLaDA/blob/main/GUIDELINES.md#:~:text=For%20the%20LLaDA,confidence%20remasking%20to%20the%20three)). For a simple POC, the random strategy is easier to implement and quite effective.)
   - After remasking, go to the next iteration with the updated sequence.
3. **Termination:** Repeat for `T` steps or until no `[MASK]` tokens remain (whichever comes first). The sequence is now fully generated. If an `<EOS>` appears, you can stop early.

This procedure is analogous to how image diffusion gradually refines a sample from pure noise. Here the “noise” is the uncertainty in masked tokens, which decreases as more tokens get fixed in place. **Figure 2(c)** of the LLaDA paper conceptually illustrates this: the model goes from a fully masked sequence to a fully unmasked one, predicting all masked tokens at each step and slowly reducing the number of masks ([[2502.09992] Large Language Diffusion Models](https://ar5iv.org/html/2502.09992v2#:~:text=Image%3A%20Refer%20to%20caption%20Figure,step%20with%20flexible%20remask%20strategies)).

**Tuning the sampling:** The number of steps `T` is a key hyperparameter. More steps = more opportunities to refine = better quality, but slower generation. In the extreme, if `T` equals the sequence length and you unmask one token per step (like an autoregressive model), you get maximum quality and AR-like speed. If `T` is much smaller than length (filling many tokens per step), you get faster but perhaps lower fidelity output. The LLaDA paper notes that this gives a natural *speed-quality trade-off* ([[2502.09992] Large Language Diffusion Models](https://ar5iv.org/html/2502.09992v2#:~:text=from%20a%20fully%20masked%20response,the%20generation%20length%20is%20also)). In their experiments, they often set `T` around half the output length (meaning ~2 tokens generated per step on average) to balance speed and quality ([[2502.09992] Large Language Diffusion Models](https://ar5iv.org/html/2502.09992v2#:~:text=For%20efficiency%2C%20we%20set%20the,in%20Table%C2%A06%2C%20for%20the%20base)). For a POC, you might try a simple schedule like: unmask a fixed percentage (say 50%) of the remaining masks each time until done. This will finish in `T ≈ log2(sequence_length)` steps. Alternatively, unmask a fixed number of tokens each step (e.g. 5 tokens per step) for linear control over `T`. 

**Sample Pseudocode for Generation:**
```python
# Assume we have a trained model and a tokenizer
prompt = "The weather today"  # example prompt
prompt_ids = tokenizer.encode(prompt)
max_gen_len = 20
# Initialize sequence: prompt + masks for remaining length
sequence = prompt_ids + [mask_token_id] * max_gen_len

T = 10  # number of refinement steps
for step in range(T):
    logits = model(torch.tensor([sequence]))[0]  # forward pass (seq_length x vocab)
    mask_positions = [i for i, tok in enumerate(sequence) if tok == mask_token_id]
    if not mask_positions:
        break  # no masks left, generation complete
    # Predict tokens for all current masks
    for pos in mask_positions:
        token_probs = softmax(logits[pos])
        predicted_token = sample_from(token_probs)  # or argmax
        sequence[pos] = predicted_token
    if step < T-1:
        # Remask some of the tokens to refine further (e.g. random half)
        for pos in mask_positions:
            if random.random() < 0.5:
                sequence[pos] = mask_token_id  # keep it masked for next round
# Decode the sequence to text, stripping any remaining masks or EOS
generated_text = tokenizer.decode([tok for tok in sequence if tok != mask_token_id])
```

This pseudocode fills *all* masks each step then randomly re-masks half for further refinement. In practice, you could directly only fill half to begin with – the strategies are equivalent in outcome. The end result is that after `T` iterations the sequence has no masks and is hopefully coherent.

**Faster sampling considerations:** The goal is to generate *as fast as autoregressive models*. With the above approach, if we set `T` proportional to sequence length, the speed is similar to an ARM. But if we can reduce `T` (say `T = 10` regardless of length) and still get good outputs, we achieve real speedup by generating many tokens per step. Recent research shows diffusion LMs can indeed generate with far fewer steps via advanced methods (e.g. distillation or better sampling) – for instance, one project reports up to *10× faster than GPT-style models* on hardware accelerators ([@Kseniase on Hugging Face: "5 New implementations of Diffusion Models

Diffusion models are widely used…"](https://huggingface.co/posts/Kseniase/475779328543857#:~:text=1,1%2C000%20tokens%2Fsec%20on%20NVIDIA%20H100s)). For your POC, focus on correctness first; you can experiment with lowering `T` once it’s working. Even a small number of steps will demonstrate parallel token generation.

## Reproducing a Minimal Working Example

Now that we understand the theory, let's outline how a practitioner can build a toy version of LLaDA:

### Dependencies and Environment
You’ll want a Python ML framework – typically **PyTorch** (as the official implementation uses PyTorch). Hugging Face’s `transformers` library can be very handy for tokenizer handling and even model scaffolding:
- **PyTorch** – for defining and training the Transformer.
- **Huggingface Transformers** – to use a pretrained tokenizer or even reuse a Transformer model class. For example, you might start from `BertConfig`/`BertModel` (which is an encoder) or `GPT2Model` (decoder) and adjust it. The official LLaDA HuggingFace model uses a custom implementation, but for a POC you can adapt an existing architecture with minimal changes.
- If training from scratch, you’ll also need a **dataset**. You could use a small text corpus (WikiText-103, OpenWebText, etc., or even a subset of Wikipedia) to train the model to imitate natural text.

No special libraries beyond standard ones are needed. The authors note that if you have a codebase for training an AR language model, you can adapt it to LLaDA with only a few lines changed ([LLaDA/README.md at main · ML-GSAI/LLaDA · GitHub](https://github.com/ML-GSAI/LLaDA/blob/main/README.md#:~:text=We%20will%20not%20provide%20the,source%20LLMs%20do)) (essentially: remove causal mask, change data corruption).

**Compute needs:** This depends on your model size and data. For a proof-of-concept, you should choose a *small model and dataset*. For example, a 50 million parameter Transformer trained on, say, 10 million tokens of text might be feasible on a single GPU in a few hours. The original LLaDA was 8 billion params on 2.3 trillion tokens ([LLaDA/README.md at main · ML-GSAI/LLaDA · GitHub](https://github.com/ML-GSAI/LLaDA/blob/main/README.md#:~:text=match%20at%20L341%204,the%20training%20stability%20of%20LLaDA)), which is **massive** – we’ll obviously not attempt anything like that. Instead:
- Use a small number of Transformer layers (e.g. 6 layers, 8 attention heads, ~256-512 hidden size).
- Limit sequence length (maybe 128 tokens for training) to keep memory manageable.
- Train on a small but rich text corpus (even a few hundred MB of text can be enough to see the model learn English structure).
- Train for a limited number of steps (this POC won’t need the full convergence or in-context learning abilities).

### Step 1: Model Initialization
Define a Transformer model that suits masked prediction. Easiest is to use a **Transformer Encoder** design:
- You can use `transformers.BertModel` which by default is a bidirectional encoder. However, BERT has some training-time baggage (like next sentence prediction) we don’t need, and it expects an attention mask input. Alternatively, you could use `GPT2Model` and just disable its causal mask. In code, for instance, set `config.attn_mask_type = 'bidirectional'` if using a library that supports it, or modify the attention mask matrix to allow full attention.
- Ensure the model’s vocabulary has a `[MASK]` token. If using a pretrained tokenizer (say GPT-2’s BPE), you might have to add a new token for `[MASK]`. The LLaDA team reserved an ID for mask (they used ID 126336, meaning their vocab was that large) ([LLaDA/GUIDELINES.md at main · ML-GSAI/LLaDA · GitHub](https://github.com/ML-GSAI/LLaDA/blob/main/GUIDELINES.md#:~:text=In%20addition%2C%20LLaDA%20designates%20a,126336)). For simplicity, you could start with a smaller vocab (perhaps 30k-50k tokens) and add one extra token for `[MASK]`.
- The model will output logits for each token position. We will train it from scratch (random initialization) since no existing model was trained with variable masking.

### Step 2: Data Preprocessing and Batching
- Use a tokenizer to convert your text data into sequences of token IDs. If sequences are longer than your model’s max length, break them up. (Unlike AR models, we don’t need to add special padding for left-to-right causality, but you might pad shorter sequences in a batch to the same length and tell the model to ignore pad positions – a standard practice.)
- **Dynamic masking:** It’s important to apply the random masking *on the fly* for each batch, so the model sees varied mask patterns. You can create a custom PyTorch `Dataset` or `DataCollator` that, in each iteration, does the masking procedure: sample `t`, compute `p_mask`, and produce the masked input and a mask indicator for loss calculation ([LLaDA/GUIDELINES.md at main · ML-GSAI/LLaDA · GitHub](https://github.com/ML-GSAI/LLaDA/blob/main/GUIDELINES.md#:~:text=p_mask%20%3D%20%281%20,None%5D.repeat%281%2C%20l)). This can be vectorized as shown in the snippet earlier.
- Each training sample yields two inputs to the model: the corrupted sequence (with `[MASK]` in some positions) and possibly an attention mask if needed (to distinguish real tokens vs padding). The target outputs are simply the original tokens. But you don’t feed targets into the model; you’ll use them to compute loss externally.

### Step 3: Training Loop
Training is very similar to any language model training:
  - Loop over batches from your dataset.
  - For each batch, perform the masking as described to get `noisy_input` and `mask_positions`.
  - Forward the `noisy_input` through the Transformer, get `logits`.
  - Compute loss = cross entropy of `logits` vs `original_tokens` at the masked positions only, with the weighting by `1/p_mask` per token ([LLaDA/GUIDELINES.md at main · ML-GSAI/LLaDA · GitHub](https://github.com/ML-GSAI/LLaDA/blob/main/GUIDELINES.md#:~:text=noisy_batch%2C%20masked_indices%2C%20p_mask%20%3D%20forward_process,logits)). (If implementing from scratch, remember to average the loss properly. The LLaDA code divides by sequence length so that each sequence contributes equally ([LLaDA/GUIDELINES.md at main · ML-GSAI/LLaDA · GitHub](https://github.com/ML-GSAI/LLaDA/blob/main/GUIDELINES.md#:~:text=noisy_batch%2C%20masked_indices%2C%20p_mask%20%3D%20forward_process,logits)).)
  - Backpropagate and optimizer step.

Using a framework like Hugging Face Trainer, you could wrap this in a custom training loop. However, since the objective is custom (due to the mask weighting), you might write the training loop manually for clarity.

Train for a few epochs (passes through your data). Monitor the loss – it should steadily decrease. You might also hold out a small validation set (masked in random ways) to ensure the model isn’t overfitting.

### Step 4: Inference (Generation) Implementation
Once the model has learned to predict masked tokens, implement the iterative decoding:
  - Decide on a context/prompt. Tokenize it. Decide how many tokens you want to generate (or use an EOS token to terminate).
  - Form the initial sequence: `[PROMPT] + [MASK] * N`. If no prompt, it’s just N masks (plus maybe a beginning-of-sequence token if your data needs one).
  - Then loop as described in the decoding approach section. On each iteration, get model logits, fill in tokens, re-mask some for further refinement.
  - One thing to be mindful of: if your model was trained with the assumption of a certain distribution of `[MASK]` tokens (including possibly none), it might sometimes not know how to handle a case where *no tokens are masked* (because then it has nothing to predict). In practice, you’ll stop when no masks left. If your loop ends with some masks still present (due to hitting max steps), you can fill them in one final time deterministically.
  - **Stop conditions:** If you included an EOS token in training (the LLaDA fine-tuning did for instruction following), you can stop generation when the model outputs `<EOS>` and leaves everything after it masked. If unconditional without EOS, just generate a fixed length or until meaninglessness.
  
Test your generation on a few prompts. With a small model, you should not expect extremely fluent paragraphs, but you might see it produce plausible short sentences. For example, you prompt it with *"The cat"* and let it continue – the model might output something like *"The cat sat on the mat."* after a few refinement steps, if it has learned basic language patterns.

### Simplifications and Approximations for POC
Because we’re limited in resources, we can simplify several aspects:
- **Model size:** Use a much smaller Transformer than the original. Even a model with ~50M parameters (roughly the size of GPT-2 small) can demonstrate the masked diffusion concept. This drastically cuts down compute. It also means you can train on a single GPU. The trade-off is that the output text quality will be basic and the model’s “knowledge” (without massive data) is limited – but that’s fine for a demo.
- **Data:** Instead of trillions of tokens, train on a small but varied corpus (for example, a few million tokens from Wikipedia or Reddit). You might also train only on a specific domain (e.g., Shakespeare texts) to get more coherent results within that domain for minimal data.
- **Training duration:** You do not need to pretrain as extensively as LLaDA. Even a day or two of training on a single GPU may be enough to get a model that forms grammatical sentences. You won’t achieve the in-context learning or reasoning abilities that large LLMs have, but you will see the model fill in masks with context-appropriate words.
- **Sampling strategy:** Start with the simpler *random remasking* strategy during generation. It’s easier to implement than confidence-based and tends to be stable ([LLaDA/GUIDELINES.md at main · ML-GSAI/LLaDA · GitHub](https://github.com/ML-GSAI/LLaDA/blob/main/GUIDELINES.md#:~:text=than%20512,when%20randomly%20remasking%20is%20used)). You could even fix a schedule (like fill 25% new tokens each step) instead of dynamically choosing which to re-mask based on confidence.
- **No classifier-free guidance or other tricks:** The LLaDA and related research sometimes use advanced tricks (e.g. an *unsupervised classifier-free guidance* to boost conditional generation ([GitHub - ML-GSAI/SMDM: Official PyTorch implementation for ICLR2025 paper "Scaling up Masked Diffusion Models on Text"](https://github.com/ML-GSAI/SMDM#:~:text=comparable%20or%20larger%20sizes,quality%20than%20ARMs%20at%20a))). For a POC, you can ignore these. They are used to improve output quality in specific tasks but are not necessary to get the model working.
- **Evaluation:** With limited resources, stick to qualitative checks. Generate a few samples, see if they make sense. If you want quantitative evaluation, you could measure perplexity on a small test set by computing the *conditional likelihood* of some text (the LLaDA code provides a `get_log_likelihood.py` for this) ([LLaDA/README.md at main · ML-GSAI/LLaDA · GitHub](https://github.com/ML-GSAI/LLaDA/blob/main/README.md#:~:text=We%20provide%20,likelihood%20evaluation%20and%20conditional%20generation)), but it’s not essential for a demo.

### Available Resources and References
Thankfully, this area has growing open-source support. Here are some pointers:
- **LLaDA Code**: The official LLaDA repository provides code for inference (generation and chat interface) and guidelines for training ([LLaDA/README.md at main · ML-GSAI/LLaDA · GitHub](https://github.com/ML-GSAI/LLaDA/blob/main/README.md#:~:text=We%20will%20not%20provide%20the,source%20LLMs%20do)) ([LLaDA/README.md at main · ML-GSAI/LLaDA · GitHub](https://github.com/ML-GSAI/LLaDA/blob/main/README.md#:~:text=0,my%20own%20LLaDA)). While they did not release the full training code for LLaDA-8B (due to its scale), the *GUIDELINES.md* includes pseudo-code snippets that we partially reproduced above, which confirm the simplicity of the changes needed ([LLaDA/GUIDELINES.md at main · ML-GSAI/LLaDA · GitHub](https://github.com/ML-GSAI/LLaDA/blob/main/GUIDELINES.md#:~:text=p_mask%20%3D%20%281%20,None%5D.repeat%281%2C%20l)) ([LLaDA/GUIDELINES.md at main · ML-GSAI/LLaDA · GitHub](https://github.com/ML-GSAI/LLaDA/blob/main/GUIDELINES.md#:~:text=noisy_batch%2C%20masked_indices%2C%20p_mask%20%3D%20forward_process,logits)).
- **SMDM (Scaling Masked Diffusion Models)**: This is an earlier work by the same authors. They open-sourced the training framework for models up to 1.1B parameters ([GitHub - ML-GSAI/SMDM: Official PyTorch implementation for ICLR2025 paper "Scaling up Masked Diffusion Models on Text"](https://github.com/ML-GSAI/SMDM#:~:text=their%20scalability%20and%20effectiveness%20in,1B%20MDM%20outperforms%20the)) ([GitHub - ML-GSAI/SMDM: Official PyTorch implementation for ICLR2025 paper "Scaling up Masked Diffusion Models on Text"](https://github.com/ML-GSAI/SMDM#:~:text=Motivated%20by%20their%20scalability%2C%20we,generation%2C%20MDMs%20provide%20a%20flexible)). You can refer to the SMDM GitHub for a complete example of training loops, data loading, and model definitions compatible with masked diffusion. SMDM’s code and model zoo (hosted on Hugging Face) include smaller pretrained models (e.g. ~170M) that you could potentially load and experiment with ([nieshen/SMDM · Hugging Face](https://huggingface.co/nieshen/SMDM#:~:text=Scaling%20law%20experiments%3A%20We%20provided,random%20sequence%20lengths%20during%20pretraining)). Using a pre-trained diffusion LM can save you training time – you could skip straight to generation if you use their weights.
- **Hugging Face Transformers integration**: As of early 2025, diffusion LMs are new, so library support is not as plug-and-play as for GPT-2/3. The LLaDA team provided their model on Hugging Face Hub (`GSAI-ML/LLaDA-8B-Base` and `…-Instruct`) along with a custom `AutoModel` loader that sets up generation ([LLaDA/README.md at main · ML-GSAI/LLaDA · GitHub](https://github.com/ML-GSAI/LLaDA/blob/main/README.md#:~:text=The%20LLaDA,employ%20the%20transformers%20to%20load)). For your own model, you might not need such complexity; a straightforward PyTorch `nn.Module` is sufficient. But you can still use Hugging Face’s tokenizers and maybe leverage their `Trainer` for convenience by supplying a custom collate function for masking.

### Feasibility with Limited Resources
**Can you create a POC with limited resources?** Yes – if you keep the scope reasonable. The core algorithm of masked diffusion is actually *less complex than many alternatives*. It’s essentially masked language modeling, which is a well-studied, easily implemented training task, combined with an iterative generation loop. The heavy demands of LLaDA came from scaling up (billions of params, trillions of tokens). For a small-scale prototype:
- You *don’t* need TPU pods or dozens of GPUs – a single modern GPU with 12–24GB memory should be enough for a model in the 100M parameter range, which is plenty to learn syntax and some semantics.
- Training time can be managed by using a smaller dataset or fewer epochs; you just want to demonstrate that the model can generate something sensible via diffusion.
- If you cannot train at all, you might attempt to use an existing MLM like BERT to simulate the process. For example, one could take a pretrained BERT and use the iterative masking technique to generate text. BERT wasn’t trained for full-sentence generation, but it might still produce short, generic sentences when used in this way (since BERT knows how to guess missing words). This could be an interesting hack to see the diffusion idea in action without training. However, results will be limited and it’s more of a curiosity. A better approach if training is not possible is to use the released smaller diffusion LM weights (like SMDM’s 170M model) and write your own sampling loop around it. The released models have already learned to generate, so you can focus on implementing the sampler.

**Which parts could be bottlenecks?** The iterative nature means generation will be slower than a single forward pass. If your model is small and sequence length short, this is negligible. But if you were to generate, say, 256 tokens in 16 steps with a 100M model, that’s 16 forward passes of 256 tokens – easily done in under a second or two on a GPU. With larger models or very long texts, you’d start to notice latency. For a POC, keep sequences fairly short (a couple of sentences).

**Potential issues to watch for:**
- The model might learn to trivially copy prompt tokens and struggle to invent new content if your training data was too small or if it overfits. Mitigate by including enough variety in training and not masking *all* tokens 100% of the time (the uniform sampling of `t` takes care of this, ensuring sometimes the model just autoencodes an almost-intact sequence and other times has to generate from scratch).
- With very small models, you might see repetitive or trivial outputs (like lots of `<EOS>` or common phrases). This is common in under-trained language models. You can experiment with sampling (use nucleus or top-k sampling on the logits) to increase diversity in generation.

## Conclusion
Building a diffusion-based language model like LLaDA is quite feasible on a small scale. By leveraging a standard Transformer encoder architecture and training it on a masked token prediction objective across varying mask ratios, you can *approximately reproduce the essence of LLaDA*. The decoding process requires implementing an iterative mask-and-fill loop, but that is also straightforward with a little coding. 

While your POC won’t rival GPT-3 in fluency or LLaDA 8B in following complex instructions, it will **demonstrate the core idea**: we can generate coherent text through a diffusion-like refinement process, rather than one token at a time. Importantly, this exercise will give you insight into how diffusion LMs operate under the hood. And as research like LLaDA has shown, when scaled up, these models become a promising alternative to autoregressive LMs, combining the strengths of bidirectional knowledge with generative power ([@Kseniase on Hugging Face: "5 New implementations of Diffusion Models

Diffusion models are widely used…"](https://huggingface.co/posts/Kseniase/475779328543857#:~:text=3.%20LLaDA%20,4o%20in%20reversal%20poetry)).

**Sources:**

- Nie *et al.* (2025), *Large Language Diffusion Models (LLaDA)* – arXiv:2502.09992 ([[2502.09992] Large Language Diffusion Models](https://ar5iv.org/html/2502.09992v2#:~:text=Image%3A%20Refer%20to%20caption%20Figure,step%20with%20flexible%20remask%20strategies)) ([[2502.09992] Large Language Diffusion Models](https://ar5iv.org/html/2502.09992v2#:~:text=LLaDA%20employs%20a%20Transformer%C2%A0,the%20entire%20input%20for%20predictions))  
- LLaDA Project Page and Code ([Large Language Diffusion Models](https://ml-gsai.github.io/LLaDA-demo/#:~:text=LLaDA%20is%20a%20masked%20diffusion,each%20step%20with%20flexible%20remasking)) ([LLaDA/GUIDELINES.md at main · ML-GSAI/LLaDA · GitHub](https://github.com/ML-GSAI/LLaDA/blob/main/GUIDELINES.md#:~:text=noisy_batch%2C%20masked_indices%2C%20p_mask%20%3D%20forward_process,logits))  
- Dongre (2025), *Diffusion Over Autoregression* (Towards AI article) ([Diffusion Over Autoregression. Explore the paper that highlights the… | by Anay Dongre | Mar, 2025 | Towards AI](https://pub.towardsai.net/diffusion-over-autoregression-5e3e4e160470?source=rss----98111c9905da---4#:~:text=LLaDA%20replaces%20the%20autoregressive%20paradigm,breaking%20free%20from%20autoregression%E2%80%99s%20constraints)) ([Diffusion Over Autoregression. Explore the paper that highlights the… | by Anay Dongre | Mar, 2025 | Towards AI](https://pub.towardsai.net/diffusion-over-autoregression-5e3e4e160470?source=rss----98111c9905da---4#:~:text=Once%20a%20sequence%20has%20been,progressively%20improving%20the%20predicted%20tokens))  
- Hugging Face Blog (2025), *5 New implementations of Diffusion Models* ([@Kseniase on Hugging Face: "5 New implementations of Diffusion Models

Diffusion models are widely used…"](https://huggingface.co/posts/Kseniase/475779328543857#:~:text=1,1%2C000%20tokens%2Fsec%20on%20NVIDIA%20H100s))